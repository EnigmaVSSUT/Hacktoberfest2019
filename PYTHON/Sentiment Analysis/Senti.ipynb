{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things from keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense,GRU,Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train = True):\n",
    "    #loading for sentiment ananlysis\n",
    "    # its like conditional operator\n",
    "    # it will return a list of reviews as text-strings and a list of corresponding sentimnts\n",
    "\n",
    "    # part of the path-name for either train or test-set\n",
    "    train_test_path = \"train\" if train else \"test\"\n",
    "\n",
    "    # base directory where the extarated data s located\n",
    "    dir_base = os.path.join(data_dir, \"aclImdb\", train_test_path)\n",
    "\n",
    "    # Filename-patterns for the data-files\n",
    "    path_pattern_pos = os.path.join(dir_base, \"pos\", \"*.txt\")\n",
    "    path_pattern_neg = os.path.join(dir_base, \"neg\", \"*.txt\")\n",
    "    \n",
    "    # Get list of all the file-paths for the data\n",
    "    path_pos = glob.glob(path_pattern_pos)\n",
    "    path_neg = glob.glob(path_pattern_neg)\n",
    "\n",
    "    # Read all the text_files\n",
    "    data_pos = [_read_text_file(path) for path in path_pos]\n",
    "    data_neg = [_read_text_file(path) for path in path_neg]\n",
    "\n",
    "    # Concatenate the positive and negative data\n",
    "    x = data_pos + data_neg\n",
    "    \n",
    "    #creating a list of sentiment for the text-data\n",
    "    y = [1.0]*len(data_pos) + [0.0] * len(data_neg)\n",
    "\n",
    "    return x,y\n",
    "def _read_text_file(path):\n",
    "    # Read and return alll the content of the text file with the given path\n",
    "\n",
    "    with open(path, 'rt', encoding = 'utf-8') as file:\n",
    "        # Read a list of string\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Concatenate to a single string.\n",
    "        text = \" \".join(lines)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  saving the data in the location\n",
    "data_dir = \"D:\\\\Ongoing Work\\\\Tensorflow\\\\NLP-Sentiment analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training and testing dataset\n",
    "x_train_text,y_train = load_data(train = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_text,y_test = load_data(train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-set size: \",len(x_train_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set size:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Test-set size: \",len(x_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "# combine into one dataset for some uses below\n",
    "data_text = x_train_text + x_test_text\n",
    "print(x_train_text[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nn cannot work directly on text-strings dataset so there is a step called tokenizer which\n",
    "# converts words to integer and is done on the dataset before it is given as input to the nn\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  it then strips it from unwanted characters such as punctuations and converts it to a lower case\n",
    "# then it bulids a vocabulary of all unique words along with varous datastructures \n",
    "# we fit the tokenizer on the entire data-set so it gathers words from both the training and test set\n",
    "\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'a': 3,\n",
       " 'of': 4,\n",
       " 'to': 5,\n",
       " 'is': 6,\n",
       " 'br': 7,\n",
       " 'in': 8,\n",
       " 'it': 9,\n",
       " 'i': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " 'film': 19,\n",
       " 'on': 20,\n",
       " 'not': 21,\n",
       " 'you': 22,\n",
       " 'are': 23,\n",
       " 'his': 24,\n",
       " 'have': 25,\n",
       " 'be': 26,\n",
       " 'one': 27,\n",
       " 'he': 28,\n",
       " 'all': 29,\n",
       " 'at': 30,\n",
       " 'by': 31,\n",
       " 'an': 32,\n",
       " 'they': 33,\n",
       " 'so': 34,\n",
       " 'who': 35,\n",
       " 'from': 36,\n",
       " 'like': 37,\n",
       " 'or': 38,\n",
       " 'just': 39,\n",
       " 'her': 40,\n",
       " 'out': 41,\n",
       " 'about': 42,\n",
       " 'if': 43,\n",
       " \"it's\": 44,\n",
       " 'has': 45,\n",
       " 'there': 46,\n",
       " 'some': 47,\n",
       " 'what': 48,\n",
       " 'good': 49,\n",
       " 'when': 50,\n",
       " 'more': 51,\n",
       " 'very': 52,\n",
       " 'up': 53,\n",
       " 'no': 54,\n",
       " 'time': 55,\n",
       " 'my': 56,\n",
       " 'even': 57,\n",
       " 'would': 58,\n",
       " 'she': 59,\n",
       " 'which': 60,\n",
       " 'only': 61,\n",
       " 'really': 62,\n",
       " 'see': 63,\n",
       " 'story': 64,\n",
       " 'their': 65,\n",
       " 'had': 66,\n",
       " 'can': 67,\n",
       " 'me': 68,\n",
       " 'well': 69,\n",
       " 'were': 70,\n",
       " 'than': 71,\n",
       " 'much': 72,\n",
       " 'we': 73,\n",
       " 'bad': 74,\n",
       " 'been': 75,\n",
       " 'get': 76,\n",
       " 'do': 77,\n",
       " 'great': 78,\n",
       " 'other': 79,\n",
       " 'will': 80,\n",
       " 'also': 81,\n",
       " 'into': 82,\n",
       " 'people': 83,\n",
       " 'because': 84,\n",
       " 'how': 85,\n",
       " 'first': 86,\n",
       " 'him': 87,\n",
       " 'most': 88,\n",
       " \"don't\": 89,\n",
       " 'made': 90,\n",
       " 'then': 91,\n",
       " 'its': 92,\n",
       " 'them': 93,\n",
       " 'make': 94,\n",
       " 'way': 95,\n",
       " 'too': 96,\n",
       " 'movies': 97,\n",
       " 'could': 98,\n",
       " 'any': 99,\n",
       " 'after': 100,\n",
       " 'think': 101,\n",
       " 'characters': 102,\n",
       " 'watch': 103,\n",
       " 'films': 104,\n",
       " 'two': 105,\n",
       " 'many': 106,\n",
       " 'seen': 107,\n",
       " 'character': 108,\n",
       " 'being': 109,\n",
       " 'never': 110,\n",
       " 'plot': 111,\n",
       " 'love': 112,\n",
       " 'acting': 113,\n",
       " 'life': 114,\n",
       " 'did': 115,\n",
       " 'best': 116,\n",
       " 'where': 117,\n",
       " 'know': 118,\n",
       " 'show': 119,\n",
       " 'little': 120,\n",
       " 'over': 121,\n",
       " 'off': 122,\n",
       " 'ever': 123,\n",
       " 'does': 124,\n",
       " 'your': 125,\n",
       " 'better': 126,\n",
       " 'end': 127,\n",
       " 'man': 128,\n",
       " 'scene': 129,\n",
       " 'still': 130,\n",
       " 'say': 131,\n",
       " 'these': 132,\n",
       " 'here': 133,\n",
       " 'scenes': 134,\n",
       " 'why': 135,\n",
       " 'while': 136,\n",
       " 'something': 137,\n",
       " 'such': 138,\n",
       " 'go': 139,\n",
       " 'through': 140,\n",
       " 'back': 141,\n",
       " 'should': 142,\n",
       " 'those': 143,\n",
       " 'real': 144,\n",
       " \"i'm\": 145,\n",
       " 'now': 146,\n",
       " 'watching': 147,\n",
       " 'thing': 148,\n",
       " \"doesn't\": 149,\n",
       " 'actors': 150,\n",
       " 'though': 151,\n",
       " 'funny': 152,\n",
       " 'years': 153,\n",
       " \"didn't\": 154,\n",
       " 'old': 155,\n",
       " '10': 156,\n",
       " 'another': 157,\n",
       " 'work': 158,\n",
       " 'before': 159,\n",
       " 'actually': 160,\n",
       " 'nothing': 161,\n",
       " 'makes': 162,\n",
       " 'look': 163,\n",
       " 'director': 164,\n",
       " 'find': 165,\n",
       " 'going': 166,\n",
       " 'same': 167,\n",
       " 'new': 168,\n",
       " 'lot': 169,\n",
       " 'every': 170,\n",
       " 'few': 171,\n",
       " 'again': 172,\n",
       " 'part': 173,\n",
       " 'cast': 174,\n",
       " 'down': 175,\n",
       " 'us': 176,\n",
       " 'things': 177,\n",
       " 'want': 178,\n",
       " 'quite': 179,\n",
       " 'pretty': 180,\n",
       " 'world': 181,\n",
       " 'horror': 182,\n",
       " 'around': 183,\n",
       " 'seems': 184,\n",
       " \"can't\": 185,\n",
       " 'young': 186,\n",
       " 'take': 187,\n",
       " 'however': 188,\n",
       " 'got': 189,\n",
       " 'thought': 190,\n",
       " 'big': 191,\n",
       " 'fact': 192,\n",
       " 'enough': 193,\n",
       " 'long': 194,\n",
       " 'both': 195,\n",
       " \"that's\": 196,\n",
       " 'give': 197,\n",
       " \"i've\": 198,\n",
       " 'own': 199,\n",
       " 'may': 200,\n",
       " 'between': 201,\n",
       " 'comedy': 202,\n",
       " 'right': 203,\n",
       " 'series': 204,\n",
       " 'action': 205,\n",
       " 'must': 206,\n",
       " 'music': 207,\n",
       " 'without': 208,\n",
       " 'times': 209,\n",
       " 'saw': 210,\n",
       " 'always': 211,\n",
       " 'original': 212,\n",
       " \"isn't\": 213,\n",
       " 'role': 214,\n",
       " 'come': 215,\n",
       " 'almost': 216,\n",
       " 'gets': 217,\n",
       " 'interesting': 218,\n",
       " 'guy': 219,\n",
       " 'point': 220,\n",
       " 'done': 221,\n",
       " \"there's\": 222,\n",
       " 'whole': 223,\n",
       " 'least': 224,\n",
       " 'far': 225,\n",
       " 'bit': 226,\n",
       " 'script': 227,\n",
       " 'minutes': 228,\n",
       " 'feel': 229,\n",
       " '2': 230,\n",
       " 'anything': 231,\n",
       " 'making': 232,\n",
       " 'might': 233,\n",
       " 'since': 234,\n",
       " 'am': 235,\n",
       " 'family': 236,\n",
       " \"he's\": 237,\n",
       " 'last': 238,\n",
       " 'probably': 239,\n",
       " 'tv': 240,\n",
       " 'performance': 241,\n",
       " 'kind': 242,\n",
       " 'away': 243,\n",
       " 'yet': 244,\n",
       " 'fun': 245,\n",
       " 'worst': 246,\n",
       " 'sure': 247,\n",
       " 'rather': 248,\n",
       " 'hard': 249,\n",
       " 'anyone': 250,\n",
       " 'girl': 251,\n",
       " 'each': 252,\n",
       " 'played': 253,\n",
       " 'day': 254,\n",
       " 'found': 255,\n",
       " 'looking': 256,\n",
       " 'woman': 257,\n",
       " 'screen': 258,\n",
       " 'although': 259,\n",
       " 'our': 260,\n",
       " 'especially': 261,\n",
       " 'believe': 262,\n",
       " 'having': 263,\n",
       " 'trying': 264,\n",
       " 'course': 265,\n",
       " 'dvd': 266,\n",
       " 'everything': 267,\n",
       " 'set': 268,\n",
       " 'goes': 269,\n",
       " 'comes': 270,\n",
       " 'put': 271,\n",
       " 'ending': 272,\n",
       " 'maybe': 273,\n",
       " 'place': 274,\n",
       " 'book': 275,\n",
       " 'shows': 276,\n",
       " 'three': 277,\n",
       " 'worth': 278,\n",
       " 'different': 279,\n",
       " 'main': 280,\n",
       " 'once': 281,\n",
       " 'sense': 282,\n",
       " 'american': 283,\n",
       " 'reason': 284,\n",
       " 'looks': 285,\n",
       " 'effects': 286,\n",
       " 'watched': 287,\n",
       " 'play': 288,\n",
       " 'true': 289,\n",
       " 'money': 290,\n",
       " 'actor': 291,\n",
       " \"wasn't\": 292,\n",
       " 'job': 293,\n",
       " 'together': 294,\n",
       " 'war': 295,\n",
       " 'someone': 296,\n",
       " 'plays': 297,\n",
       " 'instead': 298,\n",
       " 'high': 299,\n",
       " 'during': 300,\n",
       " 'said': 301,\n",
       " 'year': 302,\n",
       " 'half': 303,\n",
       " 'everyone': 304,\n",
       " 'later': 305,\n",
       " 'takes': 306,\n",
       " '1': 307,\n",
       " 'seem': 308,\n",
       " 'audience': 309,\n",
       " 'special': 310,\n",
       " 'beautiful': 311,\n",
       " 'left': 312,\n",
       " 'himself': 313,\n",
       " 'seeing': 314,\n",
       " 'john': 315,\n",
       " 'night': 316,\n",
       " 'black': 317,\n",
       " 'version': 318,\n",
       " 'shot': 319,\n",
       " 'excellent': 320,\n",
       " 'idea': 321,\n",
       " 'house': 322,\n",
       " 'mind': 323,\n",
       " 'star': 324,\n",
       " 'wife': 325,\n",
       " 'fan': 326,\n",
       " 'death': 327,\n",
       " 'used': 328,\n",
       " 'else': 329,\n",
       " 'simply': 330,\n",
       " 'nice': 331,\n",
       " 'budget': 332,\n",
       " 'poor': 333,\n",
       " 'short': 334,\n",
       " 'completely': 335,\n",
       " 'second': 336,\n",
       " \"you're\": 337,\n",
       " '3': 338,\n",
       " 'read': 339,\n",
       " 'less': 340,\n",
       " 'along': 341,\n",
       " 'top': 342,\n",
       " 'help': 343,\n",
       " 'home': 344,\n",
       " 'men': 345,\n",
       " 'either': 346,\n",
       " 'line': 347,\n",
       " 'boring': 348,\n",
       " 'dead': 349,\n",
       " 'friends': 350,\n",
       " 'kids': 351,\n",
       " 'try': 352,\n",
       " 'production': 353,\n",
       " 'enjoy': 354,\n",
       " 'camera': 355,\n",
       " 'use': 356,\n",
       " 'wrong': 357,\n",
       " 'given': 358,\n",
       " 'low': 359,\n",
       " 'classic': 360,\n",
       " 'father': 361,\n",
       " 'need': 362,\n",
       " 'full': 363,\n",
       " 'stupid': 364,\n",
       " 'next': 365,\n",
       " 'until': 366,\n",
       " 'performances': 367,\n",
       " 'school': 368,\n",
       " 'hollywood': 369,\n",
       " 'rest': 370,\n",
       " 'truly': 371,\n",
       " 'awful': 372,\n",
       " 'video': 373,\n",
       " 'couple': 374,\n",
       " 'start': 375,\n",
       " 'sex': 376,\n",
       " 'recommend': 377,\n",
       " 'women': 378,\n",
       " 'let': 379,\n",
       " 'tell': 380,\n",
       " 'terrible': 381,\n",
       " 'remember': 382,\n",
       " 'mean': 383,\n",
       " 'came': 384,\n",
       " 'getting': 385,\n",
       " 'understand': 386,\n",
       " 'perhaps': 387,\n",
       " 'moments': 388,\n",
       " 'name': 389,\n",
       " 'keep': 390,\n",
       " 'face': 391,\n",
       " 'itself': 392,\n",
       " 'wonderful': 393,\n",
       " 'playing': 394,\n",
       " 'human': 395,\n",
       " 'style': 396,\n",
       " 'small': 397,\n",
       " 'episode': 398,\n",
       " 'perfect': 399,\n",
       " 'others': 400,\n",
       " 'person': 401,\n",
       " 'doing': 402,\n",
       " 'often': 403,\n",
       " 'early': 404,\n",
       " 'stars': 405,\n",
       " 'definitely': 406,\n",
       " 'written': 407,\n",
       " 'head': 408,\n",
       " 'lines': 409,\n",
       " 'dialogue': 410,\n",
       " 'gives': 411,\n",
       " 'piece': 412,\n",
       " \"couldn't\": 413,\n",
       " 'went': 414,\n",
       " 'finally': 415,\n",
       " 'mother': 416,\n",
       " 'case': 417,\n",
       " 'title': 418,\n",
       " 'absolutely': 419,\n",
       " 'live': 420,\n",
       " 'boy': 421,\n",
       " 'yes': 422,\n",
       " 'laugh': 423,\n",
       " 'certainly': 424,\n",
       " 'liked': 425,\n",
       " 'become': 426,\n",
       " 'entertaining': 427,\n",
       " 'worse': 428,\n",
       " 'oh': 429,\n",
       " 'sort': 430,\n",
       " 'loved': 431,\n",
       " 'lost': 432,\n",
       " 'hope': 433,\n",
       " 'called': 434,\n",
       " 'picture': 435,\n",
       " 'felt': 436,\n",
       " 'overall': 437,\n",
       " 'entire': 438,\n",
       " 'mr': 439,\n",
       " 'several': 440,\n",
       " 'based': 441,\n",
       " 'supposed': 442,\n",
       " 'cinema': 443,\n",
       " 'friend': 444,\n",
       " 'guys': 445,\n",
       " 'sound': 446,\n",
       " '5': 447,\n",
       " 'problem': 448,\n",
       " 'drama': 449,\n",
       " 'against': 450,\n",
       " 'waste': 451,\n",
       " 'white': 452,\n",
       " 'beginning': 453,\n",
       " '4': 454,\n",
       " 'fans': 455,\n",
       " 'totally': 456,\n",
       " 'dark': 457,\n",
       " 'care': 458,\n",
       " 'direction': 459,\n",
       " 'humor': 460,\n",
       " 'wanted': 461,\n",
       " \"she's\": 462,\n",
       " 'seemed': 463,\n",
       " 'under': 464,\n",
       " 'game': 465,\n",
       " 'children': 466,\n",
       " 'despite': 467,\n",
       " 'lives': 468,\n",
       " 'lead': 469,\n",
       " 'guess': 470,\n",
       " 'example': 471,\n",
       " 'already': 472,\n",
       " 'final': 473,\n",
       " 'throughout': 474,\n",
       " \"you'll\": 475,\n",
       " 'evil': 476,\n",
       " 'turn': 477,\n",
       " 'becomes': 478,\n",
       " 'unfortunately': 479,\n",
       " 'able': 480,\n",
       " 'quality': 481,\n",
       " \"i'd\": 482,\n",
       " 'days': 483,\n",
       " 'history': 484,\n",
       " 'fine': 485,\n",
       " 'side': 486,\n",
       " 'wants': 487,\n",
       " 'heart': 488,\n",
       " 'horrible': 489,\n",
       " 'writing': 490,\n",
       " 'amazing': 491,\n",
       " 'b': 492,\n",
       " 'flick': 493,\n",
       " 'killer': 494,\n",
       " 'run': 495,\n",
       " 'son': 496,\n",
       " '\\x96': 497,\n",
       " 'michael': 498,\n",
       " 'works': 499,\n",
       " 'close': 500,\n",
       " \"they're\": 501,\n",
       " 'act': 502,\n",
       " 'art': 503,\n",
       " 'matter': 504,\n",
       " 'kill': 505,\n",
       " 'etc': 506,\n",
       " 'tries': 507,\n",
       " \"won't\": 508,\n",
       " 'past': 509,\n",
       " 'town': 510,\n",
       " 'turns': 511,\n",
       " 'enjoyed': 512,\n",
       " 'brilliant': 513,\n",
       " 'gave': 514,\n",
       " 'behind': 515,\n",
       " 'parts': 516,\n",
       " 'stuff': 517,\n",
       " 'genre': 518,\n",
       " 'eyes': 519,\n",
       " 'car': 520,\n",
       " 'favorite': 521,\n",
       " 'directed': 522,\n",
       " 'late': 523,\n",
       " 'hand': 524,\n",
       " 'expect': 525,\n",
       " 'soon': 526,\n",
       " 'hour': 527,\n",
       " 'obviously': 528,\n",
       " 'themselves': 529,\n",
       " 'sometimes': 530,\n",
       " 'killed': 531,\n",
       " 'actress': 532,\n",
       " 'thinking': 533,\n",
       " 'child': 534,\n",
       " 'girls': 535,\n",
       " 'viewer': 536,\n",
       " 'starts': 537,\n",
       " 'city': 538,\n",
       " 'myself': 539,\n",
       " 'decent': 540,\n",
       " 'highly': 541,\n",
       " 'stop': 542,\n",
       " 'type': 543,\n",
       " 'self': 544,\n",
       " 'god': 545,\n",
       " 'says': 546,\n",
       " 'group': 547,\n",
       " 'anyway': 548,\n",
       " 'voice': 549,\n",
       " 'took': 550,\n",
       " 'known': 551,\n",
       " 'blood': 552,\n",
       " 'kid': 553,\n",
       " 'heard': 554,\n",
       " 'happens': 555,\n",
       " 'except': 556,\n",
       " 'fight': 557,\n",
       " 'feeling': 558,\n",
       " 'experience': 559,\n",
       " 'coming': 560,\n",
       " 'slow': 561,\n",
       " 'daughter': 562,\n",
       " 'writer': 563,\n",
       " 'stories': 564,\n",
       " 'moment': 565,\n",
       " 'leave': 566,\n",
       " 'told': 567,\n",
       " 'extremely': 568,\n",
       " 'score': 569,\n",
       " 'violence': 570,\n",
       " 'police': 571,\n",
       " 'involved': 572,\n",
       " 'strong': 573,\n",
       " 'chance': 574,\n",
       " 'lack': 575,\n",
       " 'cannot': 576,\n",
       " 'hit': 577,\n",
       " 'roles': 578,\n",
       " 'hilarious': 579,\n",
       " 's': 580,\n",
       " 'wonder': 581,\n",
       " 'happen': 582,\n",
       " 'particularly': 583,\n",
       " 'ok': 584,\n",
       " 'including': 585,\n",
       " 'living': 586,\n",
       " 'save': 587,\n",
       " 'looked': 588,\n",
       " \"wouldn't\": 589,\n",
       " 'crap': 590,\n",
       " 'please': 591,\n",
       " 'simple': 592,\n",
       " 'murder': 593,\n",
       " 'cool': 594,\n",
       " 'obvious': 595,\n",
       " 'happened': 596,\n",
       " 'complete': 597,\n",
       " 'cut': 598,\n",
       " 'age': 599,\n",
       " 'serious': 600,\n",
       " 'gore': 601,\n",
       " 'attempt': 602,\n",
       " 'hell': 603,\n",
       " 'ago': 604,\n",
       " 'song': 605,\n",
       " 'shown': 606,\n",
       " 'taken': 607,\n",
       " 'english': 608,\n",
       " 'james': 609,\n",
       " 'robert': 610,\n",
       " 'david': 611,\n",
       " 'seriously': 612,\n",
       " 'released': 613,\n",
       " 'reality': 614,\n",
       " 'opening': 615,\n",
       " 'jokes': 616,\n",
       " 'interest': 617,\n",
       " 'across': 618,\n",
       " 'none': 619,\n",
       " 'hero': 620,\n",
       " 'today': 621,\n",
       " 'possible': 622,\n",
       " 'exactly': 623,\n",
       " 'alone': 624,\n",
       " 'sad': 625,\n",
       " 'brother': 626,\n",
       " 'number': 627,\n",
       " 'saying': 628,\n",
       " 'career': 629,\n",
       " \"film's\": 630,\n",
       " 'usually': 631,\n",
       " 'hours': 632,\n",
       " 'cinematography': 633,\n",
       " 'talent': 634,\n",
       " 'view': 635,\n",
       " 'yourself': 636,\n",
       " 'annoying': 637,\n",
       " 'running': 638,\n",
       " 'relationship': 639,\n",
       " 'documentary': 640,\n",
       " 'wish': 641,\n",
       " 'order': 642,\n",
       " 'huge': 643,\n",
       " 'whose': 644,\n",
       " 'shots': 645,\n",
       " 'ridiculous': 646,\n",
       " 'taking': 647,\n",
       " 'important': 648,\n",
       " 'light': 649,\n",
       " 'body': 650,\n",
       " 'middle': 651,\n",
       " 'level': 652,\n",
       " 'ends': 653,\n",
       " 'female': 654,\n",
       " 'started': 655,\n",
       " 'call': 656,\n",
       " \"i'll\": 657,\n",
       " 'husband': 658,\n",
       " 'four': 659,\n",
       " 'power': 660,\n",
       " 'major': 661,\n",
       " 'word': 662,\n",
       " 'turned': 663,\n",
       " 'opinion': 664,\n",
       " 'change': 665,\n",
       " 'mostly': 666,\n",
       " 'usual': 667,\n",
       " 'scary': 668,\n",
       " 'silly': 669,\n",
       " 'rating': 670,\n",
       " 'beyond': 671,\n",
       " 'somewhat': 672,\n",
       " 'ones': 673,\n",
       " 'happy': 674,\n",
       " 'words': 675,\n",
       " 'room': 676,\n",
       " 'knew': 677,\n",
       " 'knows': 678,\n",
       " 'country': 679,\n",
       " 'disappointed': 680,\n",
       " 'talking': 681,\n",
       " 'novel': 682,\n",
       " 'apparently': 683,\n",
       " 'non': 684,\n",
       " 'strange': 685,\n",
       " 'attention': 686,\n",
       " 'upon': 687,\n",
       " 'finds': 688,\n",
       " 'single': 689,\n",
       " 'basically': 690,\n",
       " 'cheap': 691,\n",
       " 'modern': 692,\n",
       " 'due': 693,\n",
       " 'jack': 694,\n",
       " 'television': 695,\n",
       " 'musical': 696,\n",
       " 'problems': 697,\n",
       " 'miss': 698,\n",
       " 'episodes': 699,\n",
       " 'clearly': 700,\n",
       " 'local': 701,\n",
       " '7': 702,\n",
       " 'british': 703,\n",
       " 'thriller': 704,\n",
       " 'talk': 705,\n",
       " 'events': 706,\n",
       " 'five': 707,\n",
       " 'sequence': 708,\n",
       " \"aren't\": 709,\n",
       " 'class': 710,\n",
       " 'french': 711,\n",
       " 'moving': 712,\n",
       " 'ten': 713,\n",
       " 'fast': 714,\n",
       " 'review': 715,\n",
       " 'earth': 716,\n",
       " 'tells': 717,\n",
       " 'predictable': 718,\n",
       " 'team': 719,\n",
       " 'songs': 720,\n",
       " 'comic': 721,\n",
       " 'straight': 722,\n",
       " '8': 723,\n",
       " 'whether': 724,\n",
       " 'die': 725,\n",
       " 'add': 726,\n",
       " 'dialog': 727,\n",
       " 'entertainment': 728,\n",
       " 'above': 729,\n",
       " 'sets': 730,\n",
       " 'future': 731,\n",
       " 'enjoyable': 732,\n",
       " 'appears': 733,\n",
       " 'near': 734,\n",
       " 'space': 735,\n",
       " 'easily': 736,\n",
       " 'hate': 737,\n",
       " 'soundtrack': 738,\n",
       " 'bring': 739,\n",
       " 'giving': 740,\n",
       " 'lots': 741,\n",
       " 'similar': 742,\n",
       " 'romantic': 743,\n",
       " 'george': 744,\n",
       " 'supporting': 745,\n",
       " 'release': 746,\n",
       " 'mention': 747,\n",
       " 'within': 748,\n",
       " 'filmed': 749,\n",
       " 'message': 750,\n",
       " 'sequel': 751,\n",
       " 'clear': 752,\n",
       " 'falls': 753,\n",
       " 'needs': 754,\n",
       " \"haven't\": 755,\n",
       " 'dull': 756,\n",
       " 'suspense': 757,\n",
       " 'bunch': 758,\n",
       " 'eye': 759,\n",
       " 'surprised': 760,\n",
       " 'showing': 761,\n",
       " 'tried': 762,\n",
       " 'sorry': 763,\n",
       " 'certain': 764,\n",
       " 'working': 765,\n",
       " 'easy': 766,\n",
       " 'ways': 767,\n",
       " 'theme': 768,\n",
       " 'theater': 769,\n",
       " 'named': 770,\n",
       " 'among': 771,\n",
       " \"what's\": 772,\n",
       " 'storyline': 773,\n",
       " 'monster': 774,\n",
       " 'king': 775,\n",
       " 'stay': 776,\n",
       " 'effort': 777,\n",
       " 'fall': 778,\n",
       " 'stand': 779,\n",
       " 'minute': 780,\n",
       " 'gone': 781,\n",
       " 'rock': 782,\n",
       " 'using': 783,\n",
       " '9': 784,\n",
       " 'feature': 785,\n",
       " 'comments': 786,\n",
       " 'buy': 787,\n",
       " \"'\": 788,\n",
       " 'typical': 789,\n",
       " 't': 790,\n",
       " 'editing': 791,\n",
       " 'sister': 792,\n",
       " 'avoid': 793,\n",
       " 'tale': 794,\n",
       " 'mystery': 795,\n",
       " 'deal': 796,\n",
       " 'dr': 797,\n",
       " 'doubt': 798,\n",
       " 'fantastic': 799,\n",
       " 'kept': 800,\n",
       " 'nearly': 801,\n",
       " 'feels': 802,\n",
       " 'okay': 803,\n",
       " 'subject': 804,\n",
       " 'viewing': 805,\n",
       " 'elements': 806,\n",
       " 'oscar': 807,\n",
       " 'check': 808,\n",
       " 'realistic': 809,\n",
       " 'points': 810,\n",
       " 'greatest': 811,\n",
       " 'means': 812,\n",
       " 'herself': 813,\n",
       " 'parents': 814,\n",
       " 'famous': 815,\n",
       " 'imagine': 816,\n",
       " 'rent': 817,\n",
       " 'viewers': 818,\n",
       " 'richard': 819,\n",
       " 'crime': 820,\n",
       " 'form': 821,\n",
       " 'peter': 822,\n",
       " 'actual': 823,\n",
       " 'lady': 824,\n",
       " 'general': 825,\n",
       " 'dog': 826,\n",
       " 'follow': 827,\n",
       " 'believable': 828,\n",
       " 'period': 829,\n",
       " 'red': 830,\n",
       " 'move': 831,\n",
       " 'brought': 832,\n",
       " 'material': 833,\n",
       " 'forget': 834,\n",
       " 'somehow': 835,\n",
       " 'begins': 836,\n",
       " 're': 837,\n",
       " 'reviews': 838,\n",
       " 'animation': 839,\n",
       " 'paul': 840,\n",
       " \"you've\": 841,\n",
       " 'leads': 842,\n",
       " 'weak': 843,\n",
       " 'figure': 844,\n",
       " 'surprise': 845,\n",
       " 'hear': 846,\n",
       " 'sit': 847,\n",
       " 'average': 848,\n",
       " 'open': 849,\n",
       " 'sequences': 850,\n",
       " 'atmosphere': 851,\n",
       " 'killing': 852,\n",
       " 'eventually': 853,\n",
       " 'tom': 854,\n",
       " 'learn': 855,\n",
       " 'premise': 856,\n",
       " '20': 857,\n",
       " 'wait': 858,\n",
       " 'sci': 859,\n",
       " 'deep': 860,\n",
       " 'fi': 861,\n",
       " 'expected': 862,\n",
       " 'whatever': 863,\n",
       " 'indeed': 864,\n",
       " 'particular': 865,\n",
       " 'poorly': 866,\n",
       " 'note': 867,\n",
       " 'lame': 868,\n",
       " 'dance': 869,\n",
       " 'imdb': 870,\n",
       " 'situation': 871,\n",
       " 'shame': 872,\n",
       " 'third': 873,\n",
       " 'york': 874,\n",
       " 'box': 875,\n",
       " 'truth': 876,\n",
       " 'decided': 877,\n",
       " 'free': 878,\n",
       " 'hot': 879,\n",
       " \"who's\": 880,\n",
       " 'difficult': 881,\n",
       " 'needed': 882,\n",
       " 'season': 883,\n",
       " 'acted': 884,\n",
       " 'leaves': 885,\n",
       " 'unless': 886,\n",
       " 'possibly': 887,\n",
       " 'emotional': 888,\n",
       " 'romance': 889,\n",
       " 'gay': 890,\n",
       " 'sexual': 891,\n",
       " 'boys': 892,\n",
       " 'footage': 893,\n",
       " 'write': 894,\n",
       " 'western': 895,\n",
       " 'forced': 896,\n",
       " 'credits': 897,\n",
       " 'reading': 898,\n",
       " 'memorable': 899,\n",
       " 'became': 900,\n",
       " 'doctor': 901,\n",
       " 'otherwise': 902,\n",
       " 'crew': 903,\n",
       " 'begin': 904,\n",
       " 'air': 905,\n",
       " 'de': 906,\n",
       " 'question': 907,\n",
       " 'society': 908,\n",
       " 'meet': 909,\n",
       " 'male': 910,\n",
       " 'meets': 911,\n",
       " \"let's\": 912,\n",
       " 'plus': 913,\n",
       " 'cheesy': 914,\n",
       " 'hands': 915,\n",
       " 'superb': 916,\n",
       " 'screenplay': 917,\n",
       " 'interested': 918,\n",
       " 'beauty': 919,\n",
       " 'street': 920,\n",
       " 'features': 921,\n",
       " 'masterpiece': 922,\n",
       " 'perfectly': 923,\n",
       " 'whom': 924,\n",
       " 'laughs': 925,\n",
       " 'nature': 926,\n",
       " 'stage': 927,\n",
       " 'effect': 928,\n",
       " 'forward': 929,\n",
       " 'comment': 930,\n",
       " 'nor': 931,\n",
       " 'previous': 932,\n",
       " 'badly': 933,\n",
       " 'sounds': 934,\n",
       " 'e': 935,\n",
       " 'japanese': 936,\n",
       " 'weird': 937,\n",
       " 'island': 938,\n",
       " 'personal': 939,\n",
       " 'inside': 940,\n",
       " 'quickly': 941,\n",
       " 'total': 942,\n",
       " 'keeps': 943,\n",
       " 'towards': 944,\n",
       " 'result': 945,\n",
       " 'america': 946,\n",
       " 'battle': 947,\n",
       " 'crazy': 948,\n",
       " 'worked': 949,\n",
       " 'setting': 950,\n",
       " 'incredibly': 951,\n",
       " 'background': 952,\n",
       " 'earlier': 953,\n",
       " 'mess': 954,\n",
       " 'cop': 955,\n",
       " 'writers': 956,\n",
       " 'fire': 957,\n",
       " 'copy': 958,\n",
       " 'dumb': 959,\n",
       " 'unique': 960,\n",
       " 'realize': 961,\n",
       " 'powerful': 962,\n",
       " 'lee': 963,\n",
       " 'mark': 964,\n",
       " 'business': 965,\n",
       " 'rate': 966,\n",
       " 'dramatic': 967,\n",
       " 'older': 968,\n",
       " 'pay': 969,\n",
       " 'following': 970,\n",
       " 'directors': 971,\n",
       " 'girlfriend': 972,\n",
       " 'joke': 973,\n",
       " 'plenty': 974,\n",
       " 'directing': 975,\n",
       " 'various': 976,\n",
       " 'creepy': 977,\n",
       " 'baby': 978,\n",
       " 'development': 979,\n",
       " 'appear': 980,\n",
       " 'brings': 981,\n",
       " 'front': 982,\n",
       " 'ask': 983,\n",
       " 'dream': 984,\n",
       " 'water': 985,\n",
       " 'rich': 986,\n",
       " 'admit': 987,\n",
       " 'bill': 988,\n",
       " 'apart': 989,\n",
       " 'joe': 990,\n",
       " 'fairly': 991,\n",
       " 'political': 992,\n",
       " 'leading': 993,\n",
       " 'reasons': 994,\n",
       " 'portrayed': 995,\n",
       " 'spent': 996,\n",
       " 'telling': 997,\n",
       " 'cover': 998,\n",
       " 'outside': 999,\n",
       " 'fighting': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124252"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then use the tokenizer to convert all texts in the training-set to lists of these tokens.\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,   14,  744, 3506,   45,   75,   32, 1771,   15,  153,   18,\n",
       "        110,    3, 1344,    5,  343,  143,   20,    1,  920,   12,   70,\n",
       "        281, 1228,  395,   35,  115,  267,   36,  166,    5,  368,  158,\n",
       "         38, 2058,   15,    1,  504,   88,   83,  101,    4,    1, 4339,\n",
       "         14,   39,    3,  432, 1148,  136, 8697,   42,  177,  138,   14,\n",
       "       2791,    1,  295,   20, 5276,  351,    5, 3029, 2310,    1,   38,\n",
       "       8697,   43, 3611,   26,  365,    5,  127,   53,   20,    1, 2032,\n",
       "          7,    7,   18,   48,   43,   22,   70,  358,    3, 2343,    5,\n",
       "        420,   20,    1, 2032,   15,    3, 3346,  208,    1,   22,  281,\n",
       "         66,   36,    3,  344,    1,  728,  730,    3, 3864, 1320,   20,\n",
       "          1, 1543,    3, 1293,    2,  267,   22,  281, 2734,    5,   63,\n",
       "         48,   44,   37,    5,   26, 4339,   12,    6, 2079,    7,    7,\n",
       "       3425, 2891,   35, 4446,   35,  405,   14,  297,    3,  986,  128,\n",
       "         35,   45,  267,    8,    1,  181,  366, 6951,    5,   94,    3,\n",
       "       2343,   16,    3, 7017, 3090,    5,   63,   43,   28,   67,  420,\n",
       "          8,    1, 2032,   15, 3082,  483,  208,    1,   43, 2802,   28,\n",
       "         67,   77,   48,   28,  487,   16,    3,  731, 1146,    4,  232,\n",
       "         51, 4161,    1,   20,  117,    6, 1334,   20,    1,  920,   16,\n",
       "          3,   20,   24, 4086,    5,   24,  170,  831,  117,   28,  185,\n",
       "       1562,  122,    1, 7951,  237,  358,    1,   31,    3,  100,   44,\n",
       "        407,   20,   24, 9597,  117,  911,   79,  102,  585,    3,  257,\n",
       "         31,    1,  389,    4, 5176, 2137, 4636,   32, 1222, 3303,   35,\n",
       "        189, 4287,  159, 2320,   40,  344,    2,   40, 8527, 6229, 1955,\n",
       "       4910,    2, 7720, 2618,   35,   23,  472,  328,    5,    1, 2032,\n",
       "        501, 4392,  213,  237,   21,  328,    5, 4805, 6768,   37,   28,\n",
       "        281,  115,   50,  109,  986,  117,   44,  557,   38, 2574,  505,\n",
       "         38,   26,  531,    7,    7,  136,    1,  112, 1906,  201, 5176,\n",
       "          2,  292, 1731,    5,  111,   10,  255,  114, 4541,    5,   26,\n",
       "         27,    4, 3425,  104,  117, 2557,    5,  109,    3,  202,    9,\n",
       "        276,    3, 4317,  486, 1107,    5,   24, 2347,  158,  138,   14,\n",
       "       8161,  186, 3889,   38,   15,    1,  504,    5,  119,   48,   44,\n",
       "         37,  263,  137, 4737,  159, 2320,    9,    1,  365,  254,   38,\n",
       "         20,    1,   79,  524,  232,    3,  364, 2343,   37,   29,  986,\n",
       "         83,   77,   50,   33,   89,  118,   48,    5,   77,   16,   65,\n",
       "        290,  273,   33,  142,  197,    9,    5,    1, 4339,  298,    4,\n",
       "        783,    9,   37,  290,    7,    7,   38,  273,   11,   19,   80,\n",
       "       5541,   22,    5,  343,  400])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to convert the texts in the test-set to tokens\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.27716\n",
      "2209\n"
     ]
    }
   ],
   "source": [
    "# The Recurrent Neural Network can take sequences of arbitrary length as input, but in order to use a whole batch of data,it need to have the same length so either we ensure that all are in the entire data-set have the same length, or we write a custom data-generator that ensures that it has the same length within each batch.\n",
    "#first is simpler but if we use the length of the longest sequence in the data-set, then a lot of memory is wated which is a problem in large dataset.\n",
    "# So we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
    "# First we count the number of tokens in all the sequences in the data-s\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "# The max number of tokens we will allow is set to the average plus 2 standard deviations\n",
    "max_tokens = np.mean(num_tokens)+ 2 * np.std(num_tokens)\n",
    "\n",
    "#Converting the value to int\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now its imp to decide whether to do padding or truncating pre or post\n",
    "# trucation means part of the sequence thrown away and padding means adding zeros at the front or at the end\n",
    "# here we are using re bcoz we have set that model will know the text is starting and if we will do post then there is a cjance of forgetting as so many zeros will come\n",
    "\n",
    "pad = 'pre'\n",
    "# but when we aree truncating we may loose some important information or features then we have to make compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   12,    9,  213],\n",
       "       [   0,    0,    0, ...,    5,  343,  400],\n",
       "       [   0,    0,    0, ...,    6,  179,  403],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   17,   96,   74],\n",
       "       [   0,    0,    0, ...,  260, 1219,  793],\n",
       "       [   0,    0,    0, ...,   11,    6, 1377]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set into one big matrix of integers with this shape\n",
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 544)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now inverse map\n",
    "# keras implememntation token doesnot seem to have inverse from integer tokens back to words\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper-function for converting a list of tokens back to a string of words.\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they\\'ll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it\\'s like to be homeless? That is Goddard Bolt\\'s lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet\\'s on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can\\'t step off the sidewalk. He\\'s given the nickname Pepto by a vagrant after it\\'s written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They\\'re survivors. Bolt isn\\'t. He\\'s not used to reaching mutual agreements like he once did when being rich where it\\'s fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn\\'t necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks\\' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it\\'s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don\\'t know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how well it converts \n",
    "x_train_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"or as george stated has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school work or vote for the matter most people think of the homeless as just a lost cause while worrying about things such as racism the war on iraq kids to succeed technology the or worrying if they'll be next to end up on the streets br br but what if you were given a bet to live on the streets for a month without the you once had from a home the entertainment sets a bathroom pictures on the wall a computer and everything you once treasure to see what it's like to be homeless that is lesson br br mel brooks who directs who stars as plays a rich man who has everything in the world until deciding to make a bet with a sissy rival to see if he can live in the streets for thirty days without the if succeeds he can do what he wants with a future project of making more buildings the on where is thrown on the street with a on his leg to his every move where he can't step off the sidewalk he's given the by a after it's written on his forehead where meets other characters including a woman by the name of molly ann warren an ex dancer who got divorce before losing her home and her pals sailor howard morris and teddy wilson who are already used to the streets they're survivors isn't he's not used to reaching mutual like he once did when being rich where it's fight or flight kill or be killed br br while the love connection between molly and wasn't necessary to plot i found life stinks to be one of mel films where prior to being a comedy it shows a tender side compared to his slapstick work such as blazing young frankenstein or for the matter to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money maybe they should give it to the homeless instead of using it like money br br or maybe this film will inspire you to help others\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create the rnn\n",
    "model = Sequential()\n",
    "# The first layer in the RNN is a so-called Embedding-layer which converts each integer-token into a vector of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each integer token will be converted to a vector of length 8\n",
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer also need the number of words inthe vocabulary and the length of the padded token sequence\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first recurrent layer: \n",
    "# Gated Recurrent Unit \n",
    "# Here we want an output dimensionality of 16\n",
    "model.add(GRU(16, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds the second GRU with 8 output units. This will be followed by another GRU so it must also return sequences.\n",
    "\n",
    "model.add(GRU(units=8, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds the third and final GRU with 4 output units. This will be followed by a dense-layer, so it should only give the final output of the GRU and not a whole sequence of outputs.\n",
    "model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fully-connected / dense layer which computes a value between 0.0 and 1.0 that will be used as the classification output.\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedded (Embedding)   (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 194s 8ms/step - loss: 0.5029 - acc: 0.7405 - val_loss: 0.3686 - val_acc: 0.8536\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 190s 8ms/step - loss: 0.2916 - acc: 0.8920 - val_loss: 0.3931 - val_acc: 0.8272\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 196s 8ms/step - loss: 0.2332 - acc: 0.9168 - val_loss: 0.2615 - val_acc: 0.8968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9f93f3fd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps overfitting to the training dataset\n",
    "\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 76s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# calculating its classification accuracy on the test set\n",
    "# %%time\n",
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.94%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95582455 0.4011923  0.81218326 0.9506733  0.9579177  0.31032786\n",
      " 0.95552397 0.8799609  0.9170641  0.96540195 0.21240501 0.9555901\n",
      " 0.9537462  0.9236258  0.18779394 0.96319073 0.9672125  0.68618256\n",
      " 0.9645098  0.9687168  0.82553697 0.9580447  0.9637562  0.9606963\n",
      " 0.81565493 0.9686631  0.84626734 0.07441531 0.7838693  0.9638453\n",
      " 0.95521224 0.96644306 0.95134133 0.9555901  0.9617785  0.72868216\n",
      " 0.93510616 0.95884895 0.92825264 0.9023125  0.96157026 0.9580709\n",
      " 0.03907356 0.9198529  0.79178154 0.9468232  0.9645281  0.7424391\n",
      " 0.95718867 0.9239586  0.9574195  0.58071107 0.96441287 0.9634717\n",
      " 0.95826524 0.95744187 0.9658411  0.9600708  0.95875955 0.9644537\n",
      " 0.93277055 0.9660344  0.95630336 0.96499515 0.94352305 0.8836047\n",
      " 0.8961366  0.96077037 0.96561915 0.05895564 0.9670474  0.9663078\n",
      " 0.9656525  0.05715375 0.69606876 0.9638231  0.8865112  0.91913843\n",
      " 0.96264106 0.95291173 0.9669572  0.04157428 0.88417774 0.964602\n",
      " 0.96145946 0.95691633 0.9227511  0.940911   0.805407   0.9663057\n",
      " 0.4681966  0.9282448  0.9609876  0.9654375  0.83935314 0.90125406\n",
      " 0.90111595 0.9453003  0.96340543 0.04397339 0.9649419  0.14711314\n",
      " 0.95478755 0.9604365  0.96480435 0.8699273  0.96560574 0.95048714\n",
      " 0.8671176  0.9680159  0.96589047 0.95200855 0.95766914 0.92997223\n",
      " 0.9646931  0.07115367 0.9684557  0.9542877  0.04815217 0.9512495\n",
      " 0.9613341  0.9614966  0.9679028  0.83408177 0.9534738  0.96662515\n",
      " 0.967056   0.9640675  0.9623614  0.9619731  0.9666431  0.61858\n",
      " 0.9689717  0.9168974  0.93214834 0.9677058  0.9477174  0.9319194\n",
      " 0.9658532  0.9597649  0.9586779  0.94467807 0.9682684  0.26519358\n",
      " 0.03644915 0.21676955 0.9329496  0.9568223  0.9554293  0.96803445\n",
      " 0.9682666  0.9234815  0.9656475  0.90482706 0.04405095 0.966299\n",
      " 0.9490882  0.05621231 0.9623356  0.96125513 0.9686272  0.9517246\n",
      " 0.96258867 0.8925109  0.83808756 0.9620668  0.17949146 0.9655632\n",
      " 0.08234243 0.96647215 0.87843907 0.7072721  0.04253101 0.9668928\n",
      " 0.9647413  0.96957994 0.9660926  0.9168974  0.04174482 0.44927838\n",
      " 0.84648263 0.9593751  0.27854145 0.91515845 0.95118016 0.95859724\n",
      " 0.9184826  0.9535583  0.04450745 0.93563974 0.96471345 0.943865\n",
      " 0.9648666  0.93231946 0.96404225 0.96311575 0.95914024 0.962088\n",
      " 0.9152097  0.14702456 0.8658493  0.94930816 0.9687903  0.96823186\n",
      " 0.9455977  0.81570196 0.083496   0.9499554  0.08530729 0.9584094\n",
      " 0.95667046 0.07579957 0.9624847  0.9269168  0.9663612  0.03348129\n",
      " 0.7240898  0.969413   0.9646609  0.917969   0.91272944 0.21737142\n",
      " 0.07608299 0.13928309 0.9614305  0.9546309  0.9678784  0.96508646\n",
      " 0.95262724 0.97024137 0.8436183  0.9188708  0.04250065 0.05831145\n",
      " 0.9625413  0.78561574 0.04498539 0.08017755 0.10826439 0.11086097\n",
      " 0.96440005 0.7149917  0.18992248 0.04523332 0.9168502  0.95295507\n",
      " 0.94605905 0.38252062 0.9496399  0.518691   0.5933795  0.9609592\n",
      " 0.94791853 0.7963194  0.94226885 0.96824515 0.95313525 0.96775824\n",
      " 0.96616834 0.51079327 0.9702233  0.9333483  0.7824532  0.9525274\n",
      " 0.96760845 0.75337017 0.9520159  0.6967098  0.96491885 0.8816419\n",
      " 0.96738696 0.8865905  0.19085579 0.95236707 0.9284183  0.64732265\n",
      " 0.9682035  0.9602952  0.9668688  0.96241903 0.44928643 0.9668871\n",
      " 0.96000093 0.9473793  0.9455812  0.945914   0.95590657 0.0938483\n",
      " 0.9157644  0.95364374 0.94561136 0.8626254  0.93757844 0.95094764\n",
      " 0.9075983  0.9293412  0.03977719 0.9660552  0.9644498  0.9111936\n",
      " 0.9311784  0.8919538  0.96872216 0.96174026 0.04837884 0.95234704\n",
      " 0.49849454 0.962851   0.9655365  0.96745056 0.9397297  0.63726044\n",
      " 0.96715766 0.95823956 0.95879185 0.76240325 0.919393   0.82349247\n",
      " 0.9007552  0.9308295  0.966412   0.2067393  0.96557003 0.9685407\n",
      " 0.8317     0.960263   0.78880477 0.90491515 0.96915    0.96183395\n",
      " 0.9340746  0.79755425 0.93950343 0.8507     0.9331498  0.8753918\n",
      " 0.76508075 0.9666727  0.9694285  0.9689826  0.88855165 0.04978086\n",
      " 0.5174707  0.9621656  0.9593333  0.93336046 0.9400511  0.70982\n",
      " 0.9683602  0.88563377 0.9653936  0.9618491  0.86346203 0.9643528\n",
      " 0.17246097 0.84245735 0.95593315 0.2931008  0.9479317  0.07582185\n",
      " 0.20424634 0.652038   0.8011154  0.9548662  0.9682332  0.9572852\n",
      " 0.9621271  0.89882714 0.83089477 0.9631334  0.10252921 0.08375691\n",
      " 0.94797295 0.9676371  0.96605986 0.8785465  0.9694231  0.96554506\n",
      " 0.9142386  0.06482285 0.9356121  0.88746965 0.9247184  0.8411374\n",
      " 0.9649303  0.9555332  0.62505287 0.9595036  0.9561076  0.9677702\n",
      " 0.92726916 0.9139837  0.06815665 0.06197317 0.05282328 0.15949555\n",
      " 0.05814992 0.05679679 0.9596665  0.9622686  0.2874556  0.95807415\n",
      " 0.1950589  0.89211804 0.9508467  0.9631684  0.03667848 0.9607991\n",
      " 0.9553206  0.9674288  0.90489435 0.94334936 0.64569765 0.9665061\n",
      " 0.93952036 0.9685796  0.9681427  0.04571928 0.9701441  0.8657693\n",
      " 0.9632763  0.93358725 0.9206144  0.9673351  0.9548728  0.9397195\n",
      " 0.39299488 0.9401979  0.75511605 0.9598401  0.9271312  0.8864512\n",
      " 0.22332819 0.969411   0.04246469 0.9574547  0.25788936 0.47037175\n",
      " 0.14142422 0.06710718 0.9469061  0.92810076 0.12632577 0.39436755\n",
      " 0.94271034 0.3454979  0.9673699  0.8647178  0.14419739 0.84980404\n",
      " 0.96422565 0.35010636 0.96949816 0.968311   0.2169176  0.94049037\n",
      " 0.36942476 0.9660236  0.9561791  0.9196153  0.93714195 0.38992342\n",
      " 0.96876466 0.9525197  0.96729225 0.96966    0.8514277  0.8466842\n",
      " 0.9351453  0.7284742  0.9610448  0.07036081 0.86198056 0.79212517\n",
      " 0.9318938  0.9595117  0.95375997 0.7855302  0.95485324 0.9607021\n",
      " 0.9311498  0.95985866 0.96892434 0.06949649 0.95585704 0.9574888\n",
      " 0.88979524 0.9641561  0.92879975 0.93068767 0.94172275 0.96554136\n",
      " 0.9080845  0.9464483  0.9538963  0.9512014  0.9072     0.1785025\n",
      " 0.9671202  0.9683473  0.90914744 0.9531736  0.950311   0.93321335\n",
      " 0.20491405 0.6653125  0.9636325  0.9692342  0.27893332 0.9678981\n",
      " 0.9515047  0.9571652  0.9383369  0.9376194  0.96755123 0.9165815\n",
      " 0.9652244  0.9471896  0.95843744 0.9665455  0.9490619  0.95920223\n",
      " 0.9591974  0.96276575 0.9645447  0.94504845 0.9293521  0.9412411\n",
      " 0.9646341  0.96027815 0.96176845 0.74106884 0.9244744  0.5517629\n",
      " 0.96032774 0.95587045 0.9634265  0.9644381  0.96933675 0.9693822\n",
      " 0.8700508  0.80236137 0.96969056 0.9248056  0.8896514  0.95452726\n",
      " 0.9502802  0.93792635 0.76802653 0.9492667  0.96305025 0.7107878\n",
      " 0.04347308 0.03532285 0.93664837 0.9104352  0.04536272 0.16003072\n",
      " 0.8668401  0.8764259  0.28005466 0.0741716  0.9558223  0.04945893\n",
      " 0.8515832  0.9663603  0.18735145 0.23076853 0.9529453  0.95002234\n",
      " 0.21062924 0.5570505  0.65380484 0.96211934 0.96239245 0.91928035\n",
      " 0.10332324 0.9641285  0.9689568  0.9194871  0.9663245  0.86149484\n",
      " 0.95677066 0.95596325 0.925889   0.96424294 0.91055936 0.80593264\n",
      " 0.9537694  0.93624026 0.9623985  0.8330915  0.10719033 0.89184475\n",
      " 0.9544542  0.96289307 0.9640052  0.5092654  0.9613549  0.962008\n",
      " 0.31572497 0.04681568 0.05205255 0.11859168 0.07220475 0.6363463\n",
      " 0.9346301  0.3199488  0.9649478  0.96192366 0.94562507 0.03846053\n",
      " 0.9655501  0.10691283 0.9137548  0.95499355 0.9514527  0.9261787\n",
      " 0.96143514 0.88068616 0.78851914 0.95950985 0.9642587  0.96049124\n",
      " 0.9592021  0.8140197  0.9620432  0.9592676  0.96025884 0.96471125\n",
      " 0.96978766 0.92780375 0.9571048  0.9622708  0.56863713 0.964022\n",
      " 0.9662396  0.95823294 0.9656412  0.9572316  0.95934427 0.9501069\n",
      " 0.8933884  0.96160674 0.95094347 0.09897014 0.9498425  0.96390957\n",
      " 0.9430962  0.59413326 0.5816288  0.9525607  0.92291814 0.7193743\n",
      " 0.9682228  0.9257249  0.9670029  0.8830598  0.9263767  0.9382914\n",
      " 0.9678946  0.64681315 0.967067   0.8503426  0.0390492  0.96516955\n",
      " 0.9655218  0.3024993  0.8829671  0.09714981 0.8304231  0.28816313\n",
      " 0.9554366  0.95317024 0.93330336 0.09387372 0.68374217 0.44964397\n",
      " 0.95722383 0.9096631  0.86325645 0.9570624  0.949704   0.898776\n",
      " 0.95627445 0.9694112  0.96669465 0.9543358  0.13309044 0.95453185\n",
      " 0.9130099  0.94543856 0.9555107  0.9611854  0.96740973 0.7814979\n",
      " 0.85763365 0.882936   0.9403492  0.9178856  0.9668755  0.92591536\n",
      " 0.95527774 0.9602936  0.7991593  0.9635987  0.9448548  0.9236055\n",
      " 0.9649931  0.03391993 0.9202606  0.9581059  0.9465784  0.9633293\n",
      " 0.10223733 0.92948985 0.63552433 0.11642782 0.9652686  0.9657287\n",
      " 0.953887   0.9690569  0.91701484 0.9653397  0.93327117 0.9417422\n",
      " 0.9574826  0.80070347 0.7754068  0.9678744  0.96541405 0.95663947\n",
      " 0.91253245 0.5730324  0.9669358  0.9607731  0.9521811  0.96489865\n",
      " 0.9229347  0.9622133  0.732624   0.9538011  0.90935314 0.9511684\n",
      " 0.472569   0.14866295 0.83890957 0.96611303 0.9682435  0.04477169\n",
      " 0.9126589  0.07104934 0.9683815  0.81973726 0.96588314 0.9581722\n",
      " 0.9701465  0.9647248  0.42102534 0.9519261  0.8648142  0.95550686\n",
      " 0.9642966  0.96530795 0.19189219 0.9669174  0.956769   0.09009366\n",
      " 0.04443842 0.04203586 0.95968956 0.03334718 0.23344727 0.27408865\n",
      " 0.45281482 0.96237797 0.96361935 0.9064515  0.10019679 0.19970164\n",
      " 0.966857   0.9466711  0.7956847  0.9659127  0.91882354 0.95544845\n",
      " 0.74044037 0.4805801  0.8633053  0.9563955  0.6386232  0.94718486\n",
      " 0.9692491  0.9686547  0.12577768 0.96539617 0.9632306  0.9090854\n",
      " 0.8531598  0.82395464 0.9592701  0.95895344 0.9662252  0.95515865\n",
      " 0.95913106 0.8803066  0.969992   0.9686873  0.8584095  0.917615\n",
      " 0.94795644 0.17942111 0.9575213  0.9532546  0.93980074 0.8474087\n",
      " 0.96329576 0.9659908  0.9694123  0.06402911 0.9704521  0.9680637\n",
      " 0.8825037  0.9659847  0.5720764  0.9051682  0.9602366  0.96318364\n",
      " 0.9018748  0.9457141  0.87666374 0.94660884 0.9682215  0.8132512\n",
      " 0.15180936 0.9464152  0.9480706  0.9220782  0.8108641  0.04387017\n",
      " 0.6441423  0.9663618  0.04918831 0.965474   0.6112642  0.71271867\n",
      " 0.96686035 0.9584839  0.7237752  0.92444885 0.9613392  0.91745734\n",
      " 0.9591216  0.9394156  0.93910027 0.9446338  0.9374774  0.89330375\n",
      " 0.9619287  0.9233116  0.94748366 0.958604   0.9555881  0.96985716\n",
      " 0.829266   0.9206613  0.8904374  0.9343816  0.903793   0.9587535\n",
      " 0.4526127  0.46032453 0.34048247 0.18051296 0.9694984  0.9402125\n",
      " 0.9376238  0.9672385  0.59970176 0.9699442  0.96181697 0.9699487\n",
      " 0.13378756 0.9671153  0.9514214  0.90778464 0.17822874 0.9689525\n",
      " 0.6807701  0.9070344  0.9484134  0.11422023 0.9037445  0.96180356\n",
      " 0.95157194 0.96945983 0.9594958  0.9281835  0.8529497  0.9673524\n",
      " 0.92620414 0.96057165 0.92265415 0.60152555 0.9550353  0.8760014\n",
      " 0.95354974 0.9614301  0.96727717 0.94370276 0.9203466  0.82503116\n",
      " 0.962286   0.953561   0.9389738  0.3529339  0.9664569  0.75857353\n",
      " 0.9586806  0.9603193  0.954809   0.9392416  0.96959305 0.9645442\n",
      " 0.9614802  0.28315133 0.96783876 0.93679523 0.9643016  0.94672936\n",
      " 0.3572039  0.95095617 0.0357427  0.96680784 0.9318746  0.9588773\n",
      " 0.9621639  0.9646497  0.969411   0.9318746  0.9622016  0.95327646\n",
      " 0.96202105 0.9406451  0.9672564  0.95939606 0.11215471 0.96433604\n",
      " 0.9525311  0.9635755  0.9560551  0.9562617  0.9678841  0.6702802\n",
      " 0.42831212 0.96674824 0.9697964  0.96013385 0.9635031  0.9264908\n",
      " 0.9645648  0.33364743 0.9707041  0.08328663 0.04578009 0.41189924\n",
      " 0.7384603  0.9513986  0.9563293  0.88618314 0.625421   0.23988017\n",
      " 0.90130305 0.96540225 0.96559197 0.9670845  0.9680035  0.93179053\n",
      " 0.95476955 0.08630215 0.96322244 0.96130466 0.9653316  0.9661261\n",
      " 0.93878675 0.96659577 0.8702448  0.9687891  0.9543247  0.9681425\n",
      " 0.87395126 0.94045824 0.9687311  0.96729916 0.9660059  0.96430314\n",
      " 0.07621419 0.32540545 0.95593196 0.9023326 ]\n"
     ]
    }
   ],
   "source": [
    "# to show thw misclassified text we will first calculate the predicted sentiment in the first 1000 texts in the test text\n",
    "# %%time\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "y_pred = y_pred.T[0]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred= np.array([1.0 if p>0.5 else 0.0 for p in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_true = np.array(y_test[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1,   5,  10,  14,  27,  42,  69,  73,  81,  90,  99, 101, 115,\n",
      "       118, 143, 144, 145, 154, 157, 166, 168, 172, 178, 179, 182, 188,\n",
      "       199, 206, 208, 211, 215, 221, 222, 223, 232, 233, 236, 237, 238,\n",
      "       239, 242, 243, 247, 272, 280, 287, 296, 304, 306, 321, 341, 354,\n",
      "       357, 359, 360, 370, 371, 379, 392, 393, 394, 395, 396, 397, 400,\n",
      "       402, 406, 417, 426, 432, 434, 436, 437, 438, 439, 442, 443, 445,\n",
      "       448, 451, 454, 456, 461, 471, 483, 497, 504, 508, 552, 553, 556,\n",
      "       557, 560, 561, 563, 566, 567, 570, 576, 592, 600, 601, 602, 603,\n",
      "       604, 607, 611, 613, 645, 664, 667, 669, 671, 675, 677, 688, 709,\n",
      "       714, 717, 744, 745, 749, 751, 758, 764, 767, 768, 769, 771, 772,\n",
      "       773, 774, 778, 779, 787, 794, 811, 819, 834, 839, 842, 870, 871,\n",
      "       872, 873, 882, 886, 891, 915, 925, 930, 932, 946, 954, 961, 963,\n",
      "       964, 965, 971, 979, 996, 997], dtype=int64),)\n",
      "[  1   5  10  14  27  42  69  73  81  90  99 101 115 118 143 144 145 154\n",
      " 157 166 168 172 178 179 182 188 199 206 208 211 215 221 222 223 232 233\n",
      " 236 237 238 239 242 243 247 272 280 287 296 304 306 321 341 354 357 359\n",
      " 360 370 371 379 392 393 394 395 396 397 400 402 406 417 426 432 434 436\n",
      " 437 438 439 442 443 445 448 451 454 456 461 471 483 497 504 508 552 553\n",
      " 556 557 560 561 563 566 567 570 576 592 600 601 602 603 604 607 611 613\n",
      " 645 664 667 669 671 675 677 688 709 714 717 744 745 749 751 758 764 767\n",
      " 768 769 771 772 773 774 778 779 787 794 811 819 834 839 842 870 871 872\n",
      " 873 882 886 891 915 925 930 932 946 954 961 963 964 965 971 979 996 997]\n"
     ]
    }
   ],
   "source": [
    "# -We can then get indices for all the texts that were incorrectly classified by comparing all the \"clases\" of these two arrays.\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "print(incorrect)\n",
    "\n",
    "incorrect = incorrect[0]\n",
    "print(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out of 1000 how mqny are mis classified\n",
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first misclassified text\n",
    "idx = incorrect[0]\n",
    "\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I\\'m no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralling all the same.<br /><br />The film starts with some creative opening credits (imagine a Disneyfied version of the animated opening credits of HBO\\'s \"Carnivale\" and \"Rome\"), but lumbers along slowly for its first by-the-numbers hour. Once the action moves to the U.S. Open things pick up very well. Paxton does a nice job and shows a knack for effective directorial flourishes (I loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. There\\'s some compelling character development when the British Harry Vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. He also does a good job of visually depicting what goes on in the players\\' heads under pressure. Golf, a painfully boring sport, is brought vividly alive here. Credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of London and Boston at the beginning of the twentieth century.<br /><br />You know how this is going to end not only because it\\'s based on a true story but also because films in this genre follow the same template over and over, but Paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. Despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first mis classified text is\n",
    "text = x_test_text[idx]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4011923"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_true[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data\n",
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first convert these texts to arrays of integer-tokens\n",
    "tokens = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 544)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To input texts with different lengths into the model, we also need to pad and truncate them.\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9469527 ],\n",
       "       [0.9029197 ],\n",
       "       [0.8120991 ],\n",
       "       [0.87895447],\n",
       "       [0.7920635 ],\n",
       "       [0.5935842 ],\n",
       "       [0.87700737],\n",
       "       [0.30296597]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the trained model to predict the sentiment for these texts\n",
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the embedding-layer from the model:\n",
    "layer_embedding = model.get_layer('layer_embedded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  then get the weights used for the mapping done by the embedding-layer.\n",
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the integer-token for the word 'good', which is just an index into the vocabulary.\n",
    "token_good = tokenizer.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_great = tokenizer.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00690045,  0.03755159, -0.00129658, -0.05404105,  0.04052211,\n",
       "        0.03088633,  0.01241427, -0.06749809], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15195422,  0.07776609, -0.07033638, -0.13806769,  0.10075531,\n",
       "        0.1582286 ,  0.09626482, -0.06160773], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding-vectors can be measured by different metrics, e.g. Euclidean distance or cosine distance.\n",
    "\n",
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'great':\n",
      "0.000 - great\n",
      "0.010 - tends\n",
      "0.011 - lesbians\n",
      "0.013 - poetry\n",
      "0.014 - devotion\n",
      "0.014 - loudly\n",
      "0.016 - rouge\n",
      "0.018 - creates\n",
      "0.018 - captures\n",
      "0.019 - 1972\n",
      "...\n",
      "1.982 - investigating\n",
      "1.982 - unlikeable\n",
      "1.983 - lousy\n",
      "1.983 - indians\n",
      "1.983 - moore\n",
      "1.985 - unfunny\n",
      "1.985 - scantily\n",
      "1.985 - speaking\n",
      "1.989 - overlong\n",
      "1.992 - consists\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from 'worst':\n",
      "0.000 - worst\n",
      "0.005 - forgettable\n",
      "0.005 - supposed\n",
      "0.006 - mst3k\n",
      "0.008 - flimsy\n",
      "0.008 - amateurish\n",
      "0.008 - hack\n",
      "0.008 - pepper\n",
      "0.008 - smarmy\n",
      "0.008 - obnoxious\n",
      "...\n",
      "1.991 - rookie\n",
      "1.991 - lingering\n",
      "1.991 - compositions\n",
      "1.991 - loved\n",
      "1.993 - perfect\n",
      "1.993 - twists\n",
      "1.993 - exceptional\n",
      "1.995 - impressed\n",
      "1.995 - greatest\n",
      "1.998 - 7\n"
     ]
    }
   ],
   "source": [
    "print_sorted_words('worst', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
